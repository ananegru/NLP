# -*- coding: utf-8 -*-
"""Problem0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HaeZBbAIBCHv9hxXEXszxkQ15zUeYPRC
"""

import nltk

nltk.download('brown')
from nltk.corpus import brown

# List of unique words sorted by descending frequency for the entire corpus
corpus_words = brown.words()

fdist = nltk.FreqDist(corpus_words)

unique_words_sorted = sorted(set(corpus_words), key=lambda w: fdist[w], reverse=True)

print(unique_words_sorted)

# Check genres
genres = brown.categories()
print(genres)

genres = ['news', 'romance']
for genre in genres:
    # Get a list of all words in the genre
    words = brown.words(categories=genre)
    # Compute the frequency distribution of words
    freq_dist = nltk.FreqDist(words)
    # Get a list of unique words sorted by descending frequency
    unique_words_sorted_genres = sorted(set(words), key=lambda word: -freq_dist[word])
    print(unique_words_sorted_genres)

#output: ['the', ',', '.', 'of', 'and', 'to', 'a', 'in', 'that', 'is', 'was', 'for', '``', "''", 'The']...

# Get all words in the genre
words = brown.words()

# Get all sentences in the genre
sentences = brown.sents()

# Count the number of tokens (words and punctuation)
num_tokens = len(words)

# Count the number of types (unique words)
num_types = len(set(words))

# Count the number of words (excluding punctuation)
num_words = len([word for word in words if word.isalpha()])

# Compute the average number of words per sentence
avg_words_per_sentence = num_words / len(sentences)

# Compute the average word length
avg_word_length = sum(len(word) for word in words) / num_words

# Print the results
print(f"Number of tokens: {num_tokens}")
print(f"Number of types: {num_types}")
print(f"Number of words: {num_words}")
print(f"Average number of words per sentence: {avg_words_per_sentence:.2f}")
print(f"Average word length: {avg_word_length:.2f}")

#output:
#Number of tokens: 1161192
#Number of types: 56057
#Number of words: 981716
#Average number of words per sentence: 17.12
#Average word length: 5.06

nltk.download()

# POS tagger on dataset & identification of 10 most frequent POS tags
words = brown.words()

# Run the default POS tagger on the words
pos_tags = nltk.pos_tag(words)

# Count the frequency of each POS tag
freq_dist = nltk.FreqDist(tag for (word, tag) in pos_tags)

# Print the ten most frequent POS tags
print("Ten most frequent POS tags:")
for tag, count in freq_dist.most_common(10):
    print(f"{tag}: {count}")

#output:
#Ten most frequent POS tags:
#NN: 160733
#IN: 135982
#DT: 116201
#JJ: 81065
#NNP: 70646
#,: 58334
#.: 55635
#NNS: 55433
#VBD: 46923
#RB: 46134

import matplotlib.pyplot as plt

# Get the frequency distribution for the whole corpus
fdist_corpus = nltk.FreqDist(brown.words())

# Get the frequency distributions for two genres of your choice
fdist_genre1 = nltk.FreqDist(brown.words(categories='news'))
fdist_genre2 = nltk.FreqDist(brown.words(categories='romance'))

# Get the frequency lists for each distribution
freq_list_corpus = list(fdist_corpus.values())
freq_list_genre1 = list(fdist_genre1.values())
freq_list_genre2 = list(fdist_genre2.values())

# Plot the frequency curves for each distribution
plt.figure(figsize=(10, 5))
plt.plot(freq_list_corpus, label='corpus')
plt.plot(freq_list_genre1, label='news')
plt.plot(freq_list_genre2, label='romance')
plt.legend()
plt.title('Frequency Curves for Brown Corpus and Two Genres')
plt.xlabel('Frequency List Position')
plt.ylabel('Frequency')

# Plot the same curves with log-log axes
plt.figure(figsize=(10, 5))
plt.loglog(freq_list_corpus, label='corpus')
plt.loglog(freq_list_genre1, label='news')
plt.loglog(freq_list_genre2, label='romance')
plt.legend()
plt.title('Log-Log Frequency Curves for Brown Corpus and Two Genres')
plt.xlabel('Frequency List Position (log scale)')
plt.ylabel('Frequency (log scale)')

plt.show()
